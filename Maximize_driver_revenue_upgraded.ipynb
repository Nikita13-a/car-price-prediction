# Upgraded Project: Optimizing Driver Revenue with Statistical Regression Modeling
# Run this script in Python (e.g., python maximize_driver_revenue_upgraded.py)
# Intermediate upgrades: Multiple regression, stats, residuals, polynomial features.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro, pearsonr

np.random.seed(42)

# Step 1: Generate Synthetic Dataset
n = 200
distance = np.random.uniform(5, 50, n)
time = np.random.uniform(10, 120, n)
fare = 10 + 0.5 * distance + 0.2 * time + np.random.normal(0, 5, n)
driver_exp = np.random.randint(1, 10, n)
traffic = np.random.choice([0, 1], n)
revenue = 50 + 0.8 * fare + 2 * driver_exp - 5 * traffic + np.random.normal(0, 10, n)

df = pd.DataFrame({
    'Distance': distance, 'Time': time, 'Fare': fare,
    'Driver_Experience': driver_exp, 'Traffic_Level': traffic, 'Revenue': revenue
})

print("Dataset Head:")
print(df.head())

# Step 2: Descriptive Statistics
print("\n=== Descriptive Statistics ===")
print(df.describe())
print("Variance of Revenue:", df['Revenue'].var())

# Step 3: Correlations
corr_matrix = df.corr()
print("\n=== Correlation Matrix ===")
print(corr_matrix)
corr_fare_rev, p_fare = pearsonr(df['Fare'], df['Revenue'])
print(f"Pearson's r (Fare vs. Revenue): {corr_fare_rev:.2f}, p-value: {p_fare:.3f}")

plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Step 4: Model Building
X = df[['Fare', 'Driver_Experience', 'Traffic_Level']]
y = df['Revenue']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
print("\n=== Model Coefficients ===")
print(f"Intercept: {model.intercept_:.2f}")
for i, coef in enumerate(model.coef_):
    print(f"Coefficient for {X.columns[i]}: {coef:.2f}")

# Step 5: Evaluation
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"\nMSE: {mse:.2f}, R-squared: {r2:.2f}, CV R-squared: {cv_scores.mean():.2f}")

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.title('Actual vs. Predicted Revenue')
plt.show()

# Step 6: Statsmodels Analysis
X_train_sm = sm.add_constant(X_train)
model_sm = sm.OLS(y_train, X_train_sm).fit()
print("\n=== Statsmodels Summary ===")
print(model_sm.summary())

# Step 7: Residual Analysis
residuals = y_train - model.predict(X_train)
plt.figure(figsize=(12, 5))
plt.subplot(1, 3, 1)
plt.scatter(model.predict(X_train), residuals)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuals vs. Fitted')
plt.subplot(1, 3, 2)
sm.qqplot(residuals, line='45', fit=True)
plt.title('Q-Q Plot')
plt.subplot(1, 3, 3)
plt.hist(residuals, bins=20)
plt.title('Residual Histogram')
plt.show()

stat, p_value = shapiro(residuals)
print(f"Shapiro-Wilk: Statistic={stat:.3f}, p-value={p_value:.3f}")
if p_value > 0.05:
    print("Residuals normal.")
else:
    print("Residuals not normal.")

# Step 8: Polynomial Regression
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
X_train_poly, X_test_poly = train_test_split(X_poly, test_size=0.2, random_state=42)
model_poly = LinearRegression().fit(X_train_poly, y_train)
y_pred_poly = model_poly.predict(X_test_poly)
r2_poly = r2_score(y_test, y_pred_poly)
print(f"\nPolynomial R-squared: {r2_poly:.2f}")

print("\n=== Insights ===")
print("Statistics used: Descriptive stats, correlations, t-tests, R-squared, CIs, residuals.")
print("Revenue insights: Increases with fare/experience, decreases with traffic.")
